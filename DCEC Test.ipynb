{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 12:34:21.606880: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-28 12:34:22.153935: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-02-28 12:34:22.153974: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-02-28 12:34:22.153978: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 12:34:22.682252: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-28 12:34:22.711563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-28 12:34:22.711822: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "%run defs.py\n",
    "import os\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Get list of available GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"GPUs Available:\", gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Training Set and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "spikes1_detected = np.load('Spikes1_det_shift.npy')\n",
    "spikes1_class = np.load('Spikes1_class_shift.npy')\n",
    "spikes2_detected = np.load('Spikes2_det_shift.npy')\n",
    "spikes2_class = np.load('Spikes2_class_shift.npy')\n",
    "spikes3_detected = np.load('Spikes3_det_shift.npy')\n",
    "spikes3_class = np.load('Spikes3_class_shift.npy')\n",
    "spikes4_detected = np.load('Spikes4_det_shift.npy')\n",
    "spikes4_class = np.load('Spikes4_class_shift.npy')\n",
    "spikes5_detected = np.load('Spikes5_det_shift.npy')\n",
    "spikes5_class = np.load('Spikes5_class_shift.npy')\n",
    "spikes6_detected = np.load('Spikes6_det_shift.npy')\n",
    "spikes6_class = np.load('Spikes6_class_shift.npy')\n",
    "spikes7_detected = np.load('Spikes7_det_shift.npy')\n",
    "spikes7_class = np.load('Spikes7_class_shift.npy')\n",
    "spikes8_detected = np.load('Spikes8_det_shift.npy')\n",
    "spikes8_class = np.load('Spikes8_class_shift.npy')\n",
    "spikes9_detected = np.load('Spikes9_det_shift.npy')\n",
    "spikes9_class = np.load('Spikes9_class_shift.npy')\n",
    "spikes10_detected = np.load('Spikes10_det_shift.npy')\n",
    "spikes10_class = np.load('Spikes10_class_shift.npy')\n",
    "spikes11_detected = np.load('Spikes11_det_shift.npy')\n",
    "spikes11_class = np.load('Spikes11_class_shift.npy')\n",
    "spikes12_detected = np.load('Spikes12_det_shift.npy')\n",
    "spikes12_class = np.load('Spikes12_class_shift.npy')\n",
    "spikes13_detected = np.load('Spikes13_det_shift.npy')\n",
    "spikes13_class = np.load('Spikes13_class_shift.npy')\n",
    "spikes14_detected = np.load('Spikes14_det_shift.npy')\n",
    "spikes14_class = np.load('Spikes14_class_shift.npy')\n",
    "spikes15_detected = np.load('Spikes15_det_shift.npy')\n",
    "spikes15_class = np.load('Spikes15_class_shift.npy')\n",
    "spikes16_detected = np.load('Spikes16_det_shift.npy')\n",
    "spikes16_class = np.load('Spikes16_class_shift.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 0 0 2 1 0 1 1]\n",
      "[5 5 5 4 5 4 3 5 5 5]\n",
      "[8 7 8 8 7 8 7 6 6 6]\n",
      "[11 11 11  9  9 10 10  9  9  9]\n"
     ]
    }
   ],
   "source": [
    "#Import Test Data\n",
    "x1 = spikes1_detected\n",
    "x1 = np.concatenate((x1, spikes2_detected))\n",
    "x1 = np.concatenate((x1, spikes3_detected))\n",
    "x1 = np.concatenate((x1, spikes4_detected))\n",
    "y1 = spikes1_class\n",
    "y1 = np.append(y1, spikes2_class)\n",
    "y1 = np.append(y1, spikes3_class)\n",
    "y1 = np.append(y1, spikes4_class)\n",
    "#y1 = np.array(y1)\n",
    "\n",
    "x2 = spikes5_detected\n",
    "x2 = np.concatenate((x2, spikes6_detected))\n",
    "x2 = np.concatenate((x2, spikes7_detected))\n",
    "x2 = np.concatenate((x2, spikes8_detected))\n",
    "y2 = spikes5_class\n",
    "y2 = np.append(y2, spikes6_class)\n",
    "y2 = np.append(y2, spikes7_class)\n",
    "y2 = np.append(y2, spikes8_class)\n",
    "#y2 = np.array(y2)\n",
    "\n",
    "x3 = spikes9_detected\n",
    "x3 = np.concatenate((x3, spikes10_detected))\n",
    "x3 = np.concatenate((x3, spikes11_detected))\n",
    "x3 = np.concatenate((x3, spikes12_detected))\n",
    "y3 = spikes9_class\n",
    "y3 = np.append(y3, spikes10_class)\n",
    "y3 = np.append(y3, spikes11_class)\n",
    "y3 = np.append(y3, spikes12_class)\n",
    "#y3 = np.array(y3)\n",
    "\n",
    "x4 = spikes13_detected\n",
    "x4 = np.concatenate((x4, spikes14_detected))\n",
    "x4 = np.concatenate((x4, spikes15_detected))\n",
    "x4 = np.concatenate((x4, spikes16_detected))\n",
    "y4 = spikes13_class\n",
    "y4 = np.append(y4, spikes14_class)\n",
    "y4 = np.append(y4, spikes15_class)\n",
    "y4 = np.append(y4, spikes16_class)\n",
    "#y4 = np.array(y4)\n",
    "\n",
    "y1[y1 == 3]=0\n",
    "y2[y2 == 3]=5\n",
    "y2[y2 == 2]=4\n",
    "y2[y2 == 1]=3\n",
    "y3[y3 == 3]=8\n",
    "y3[y3 == 2]=7\n",
    "y3[y3 == 1]=6\n",
    "y4[y4 == 3]=11\n",
    "y4[y4 == 2]=10\n",
    "y4[y4 == 1]=9\n",
    "\n",
    "print(y1[0:10])\n",
    "print(y2[0:10])\n",
    "print(y3[0:10])\n",
    "print(y4[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_temp = x1\n",
    "x_temp= np.concatenate((x_temp, x2))\n",
    "x_temp= np.concatenate((x_temp, x3))\n",
    "x_temp= np.concatenate((x_temp, x4))\n",
    "y_temp = y1\n",
    "y_temp = np.append(y_temp, y2)\n",
    "y_temp = np.append(y_temp, y3)\n",
    "y_temp = np.append(y_temp, y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x,test_x,y,test_y = train_test_split(x_temp,y_temp,test_size=0.3,random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCEC Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Libraries\n",
    "import numpy as np\n",
    "from time import time\n",
    "from matplotlib import pyplot as plt\n",
    "import gzip\n",
    "%matplotlib inline\n",
    "\n",
    "# Keras Core Components\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# Keras Models and Layers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import (\n",
    "    Input, Layer, InputSpec, Lambda, Dense, Flatten, Reshape, Dropout, BatchNormalization, Activation, GaussianNoise, Rescaling,\n",
    "    Conv1D, Conv1DTranspose, MaxPooling1D, UpSampling1D\n",
    ")\n",
    "\n",
    "# Keras Optimizers and Callbacks\n",
    "from keras.optimizers import Adadelta, RMSprop, SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Machine Learning Utilities\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score, accuracy_score\n",
    "\n",
    "# QKeras for quantization (if needed)\n",
    "from qkeras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Autoencoder\n",
    "def CAE(input_shape=(64, 1), filters=[32, 64, 128, 12], qconfig=[1,8,1,8,1]):\n",
    "    \n",
    "    num_wbits = qconfig[0]   #weight bits\n",
    "    num_bits  = qconfig[1]   #relu bits (input bits)\n",
    "    num_int   = qconfig[2]   #relu integer bits (input integer bits)\n",
    "    num_abits = qconfig[3]   #quantization bits (ADC bits)\n",
    "    num_aint  = qconfig[4]   #quantization integer bits\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape))\n",
    "    model.add(QConv1D(filters[0], 2, strides=1, padding='same', kernel_quantizer=quantized_bits(num_wbits, 0, 1, False, alpha = 2), use_bias = False, name='conv1'))\n",
    "    model.add(GaussianNoise(stddev=0.2))\n",
    "    model.add(Rescaling(1/filters[0],offset=0.0,))\n",
    "    model.add(QConv1D(filters[1], 2, strides=1, padding='same', kernel_quantizer=quantized_bits(num_wbits, 0, 1, False, alpha = 2), use_bias = False, name='conv2'))\n",
    "    model.add(GaussianNoise(stddev=0.2))\n",
    "    model.add(Rescaling(1/filters[1],offset=0.0,))\n",
    "    model.add(QConv1D(filters[2], 2, strides=1, padding='same', kernel_quantizer=quantized_bits(num_wbits, 0, 1, False, alpha = 2), use_bias = False, name='conv3'))\n",
    "    model.add(GaussianNoise(stddev=0.2))\n",
    "    model.add(Rescaling(1/filters[2],offset=0.0,))\n",
    "    model.add(Flatten())\n",
    "    model.add(QDense(units=filters[3], kernel_quantizer=quantized_bits(num_wbits, 0, 1, False, alpha = 0.125), use_bias = False, name='embedding'))\n",
    "    model.add(QActivation(quantized_bits(num_abits, num_aint, 1, alpha = 0.25)))\n",
    "    \n",
    "    model.add(Dense(units=filters[2]*int(input_shape[0]/1), activation='relu'))\n",
    "    model.add(Reshape((int(input_shape[0]/1), filters[2])))\n",
    "    model.add(Conv1DTranspose(filters[1], 2, strides=1, padding='same', activation='relu', name='deconv3'))\n",
    "    model.add(Conv1DTranspose(filters[0], 2, strides=1, padding='same', activation='relu', name='deconv2'))\n",
    "    model.add(Conv1DTranspose(input_shape[1], 2, strides=1, padding='same', name='deconv1'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(n_clusters=12, batch_size=32, maxiter=200.0, gamma=0.1, update_interval=140, tol=0.001, cae_weights=None, save_dir='results/temp')\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1 (Conv1D)              (None, 64, 48)            96        \n",
      "                                                                 \n",
      " conv2 (Conv1D)              (None, 64, 32)            1568      \n",
      "                                                                 \n",
      " conv3 (Conv1D)              (None, 64, 32)            1056      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " embedding (Dense)           (None, 5)                 10245     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              12288     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 64, 32)            0         \n",
      "                                                                 \n",
      " deconv3 (Conv1DTranspose)   (None, 64, 32)            1056      \n",
      "                                                                 \n",
      " deconv2 (Conv1DTranspose)   (None, 64, 48)            1584      \n",
      "                                                                 \n",
      " deconv1 (Conv1DTranspose)   (None, 64, 1)             49        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,942\n",
      "Trainable params: 27,942\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " conv1_input (InputLayer)       [(None, 64, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " conv1 (Conv1D)                 (None, 64, 48)       96          ['conv1_input[0][0]']            \n",
      "                                                                                                  \n",
      " conv2 (Conv1D)                 (None, 64, 32)       1568        ['conv1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv3 (Conv1D)                 (None, 64, 32)       1056        ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2048)         0           ['conv3[0][0]']                  \n",
      "                                                                                                  \n",
      " embedding (Dense)              (None, 5)            10245       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 2048)         12288       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 64, 32)       0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " deconv3 (Conv1DTranspose)      (None, 64, 32)       1056        ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " deconv2 (Conv1DTranspose)      (None, 64, 48)       1584        ['deconv3[0][0]']                \n",
      "                                                                                                  \n",
      " clustering (ClusteringLayer)   (None, 12)           60          ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " deconv1 (Conv1DTranspose)      (None, 64, 1)        49          ['deconv2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28,002\n",
      "Trainable params: 28,002\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Learning rate:  0.001\n",
      "Update interval 140\n",
      "Save interval 50\n",
      "...pretraining CAE using default hyper-parameters:\n",
      "   optimizer='adam';   epochs=200\n",
      "...Pretraining...\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 12:42:34.540953: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155/155 [==============================] - 3s 4ms/step - loss: 0.0159\n",
      "Epoch 2/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1987e-04\n",
      "Epoch 3/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1990e-04\n",
      "Epoch 4/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1991e-04\n",
      "Epoch 5/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1994e-04\n",
      "Epoch 6/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.2005e-04\n",
      "Epoch 7/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1996e-04\n",
      "Epoch 8/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1995e-04\n",
      "Epoch 9/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.2001e-04\n",
      "Epoch 10/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1993e-04\n",
      "Epoch 11/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.2005e-04\n",
      "Epoch 12/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1999e-04\n",
      "Epoch 13/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1991e-04\n",
      "Epoch 14/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1987e-04\n",
      "Epoch 15/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1997e-04\n",
      "Epoch 16/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1993e-04\n",
      "Epoch 17/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1980e-04\n",
      "Epoch 18/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1960e-04\n",
      "Epoch 19/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1976e-04\n",
      "Epoch 20/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1921e-04\n",
      "Epoch 21/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1873e-04\n",
      "Epoch 22/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1818e-04\n",
      "Epoch 23/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1785e-04\n",
      "Epoch 24/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1640e-04\n",
      "Epoch 25/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1454e-04\n",
      "Epoch 26/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1109e-04\n",
      "Epoch 27/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.0563e-04\n",
      "Epoch 28/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 9.8025e-05\n",
      "Epoch 29/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 9.1797e-05\n",
      "Epoch 30/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 9.2327e-05\n",
      "Epoch 31/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 6.9581e-05\n",
      "Epoch 32/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 6.9216e-05\n",
      "Epoch 33/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 6.8853e-05\n",
      "Epoch 34/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 6.6768e-05\n",
      "Epoch 35/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 6.1576e-05\n",
      "Epoch 36/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 5.8571e-05\n",
      "Epoch 37/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 5.6632e-05\n",
      "Epoch 38/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 5.1248e-05\n",
      "Epoch 39/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 4.8381e-05\n",
      "Epoch 40/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 4.8851e-05\n",
      "Epoch 41/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 4.8122e-05\n",
      "Epoch 42/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 4.6747e-05\n",
      "Epoch 43/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 4.5782e-05\n",
      "Epoch 44/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 4.3987e-05\n",
      "Epoch 45/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 4.4091e-05\n",
      "Epoch 46/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 4.4030e-05\n",
      "Epoch 47/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 4.0361e-05\n",
      "Epoch 48/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 4.0710e-05\n",
      "Epoch 49/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 3.6618e-05\n",
      "Epoch 50/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 3.4848e-05\n",
      "Epoch 51/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 3.9991e-05\n",
      "Epoch 52/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 3.1614e-05\n",
      "Epoch 53/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 3.5024e-05\n",
      "Epoch 54/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 3.3558e-05\n",
      "Epoch 55/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 3.3691e-05\n",
      "Epoch 56/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 3.2584e-05\n",
      "Epoch 57/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 3.3313e-05\n",
      "Epoch 58/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 3.3798e-05\n",
      "Epoch 59/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 3.1258e-05\n",
      "Epoch 60/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 3.2420e-05\n",
      "Epoch 61/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 3.2005e-05\n",
      "Epoch 62/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 3.1526e-05\n",
      "Epoch 63/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 3.2441e-05\n",
      "Epoch 64/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 3.1506e-05\n",
      "Epoch 65/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 3.1127e-05\n",
      "Epoch 66/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 3.0504e-05\n",
      "Epoch 67/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 3.1068e-05\n",
      "Epoch 68/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 3.0871e-05\n",
      "Epoch 69/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 3.0584e-05\n",
      "Epoch 70/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 2.9457e-05\n",
      "Epoch 71/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 2.9878e-05\n",
      "Epoch 72/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 2.9073e-05\n",
      "Epoch 73/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 2.8212e-05\n",
      "Epoch 74/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 2.8423e-05\n",
      "Epoch 75/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 2.7446e-05\n",
      "Epoch 76/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 2.6087e-05\n",
      "Epoch 77/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 2.5660e-05\n",
      "Epoch 78/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 2.4572e-05\n",
      "Epoch 79/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 2.4175e-05\n",
      "Epoch 80/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 2.2673e-05\n",
      "Epoch 81/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 2.1827e-05\n",
      "Epoch 82/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 2.1704e-05\n",
      "Epoch 83/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 2.1477e-05\n",
      "Epoch 84/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 2.0289e-05\n",
      "Epoch 85/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 2.1190e-05\n",
      "Epoch 86/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.9908e-05\n",
      "Epoch 87/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 2.0638e-05\n",
      "Epoch 88/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.9913e-05\n",
      "Epoch 89/200\n",
      "155/155 [==============================] - 1s 3ms/step - loss: 1.9204e-05\n",
      "Epoch 90/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.8836e-05\n",
      "Epoch 91/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.8854e-05\n",
      "Epoch 92/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.8938e-05\n",
      "Epoch 93/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.8592e-05\n",
      "Epoch 94/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.8008e-05\n",
      "Epoch 95/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.7974e-05\n",
      "Epoch 96/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.7858e-05\n",
      "Epoch 97/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.7247e-05\n",
      "Epoch 98/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.7314e-05\n",
      "Epoch 99/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.7318e-05\n",
      "Epoch 100/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.6925e-05\n",
      "Epoch 101/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.6718e-05\n",
      "Epoch 102/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.6538e-05\n",
      "Epoch 103/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.6899e-05\n",
      "Epoch 104/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.5837e-05\n",
      "Epoch 105/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.6033e-05\n",
      "Epoch 106/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.5890e-05\n",
      "Epoch 107/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.5510e-05\n",
      "Epoch 108/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.5346e-05\n",
      "Epoch 109/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.5122e-05\n",
      "Epoch 110/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.4829e-05\n",
      "Epoch 111/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.5270e-05\n",
      "Epoch 112/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.4408e-05\n",
      "Epoch 113/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.4710e-05\n",
      "Epoch 114/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.4230e-05\n",
      "Epoch 115/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.3996e-05\n",
      "Epoch 116/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.3840e-05\n",
      "Epoch 117/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.3790e-05\n",
      "Epoch 118/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.3626e-05\n",
      "Epoch 119/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.3689e-05\n",
      "Epoch 120/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.3711e-05\n",
      "Epoch 121/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.3173e-05\n",
      "Epoch 122/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.3913e-05\n",
      "Epoch 123/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.3014e-05\n",
      "Epoch 124/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.3206e-05\n",
      "Epoch 125/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.2954e-05\n",
      "Epoch 126/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.2856e-05\n",
      "Epoch 127/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.3083e-05\n",
      "Epoch 128/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.3022e-05\n",
      "Epoch 129/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.2751e-05\n",
      "Epoch 130/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.2821e-05\n",
      "Epoch 131/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.2825e-05\n",
      "Epoch 132/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.2526e-05\n",
      "Epoch 133/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.2455e-05\n",
      "Epoch 134/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.2553e-05\n",
      "Epoch 135/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.2640e-05\n",
      "Epoch 136/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.2488e-05\n",
      "Epoch 137/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.2447e-05\n",
      "Epoch 138/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.2602e-05\n",
      "Epoch 139/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.2481e-05\n",
      "Epoch 140/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.2328e-05\n",
      "Epoch 141/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.2148e-05\n",
      "Epoch 142/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.2224e-05\n",
      "Epoch 143/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.2343e-05\n",
      "Epoch 144/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.2166e-05\n",
      "Epoch 145/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1973e-05\n",
      "Epoch 146/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.2286e-05\n",
      "Epoch 147/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1792e-05\n",
      "Epoch 148/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.2144e-05\n",
      "Epoch 149/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1884e-05\n",
      "Epoch 150/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1819e-05\n",
      "Epoch 151/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.2135e-05\n",
      "Epoch 152/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1744e-05\n",
      "Epoch 153/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1811e-05\n",
      "Epoch 154/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1826e-05\n",
      "Epoch 155/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1599e-05\n",
      "Epoch 156/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1819e-05\n",
      "Epoch 157/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1622e-05\n",
      "Epoch 158/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1729e-05\n",
      "Epoch 159/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1699e-05\n",
      "Epoch 160/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1707e-05\n",
      "Epoch 161/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1636e-05\n",
      "Epoch 162/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1405e-05\n",
      "Epoch 163/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1659e-05\n",
      "Epoch 164/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1575e-05\n",
      "Epoch 165/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1435e-05\n",
      "Epoch 166/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1284e-05\n",
      "Epoch 167/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1500e-05\n",
      "Epoch 168/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1219e-05\n",
      "Epoch 169/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1461e-05\n",
      "Epoch 170/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1298e-05\n",
      "Epoch 171/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1642e-05\n",
      "Epoch 172/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1157e-05\n",
      "Epoch 173/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1206e-05\n",
      "Epoch 174/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1304e-05\n",
      "Epoch 175/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1320e-05\n",
      "Epoch 176/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1365e-05\n",
      "Epoch 177/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1166e-05\n",
      "Epoch 178/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1119e-05\n",
      "Epoch 179/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1117e-05\n",
      "Epoch 180/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1213e-05\n",
      "Epoch 181/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1074e-05\n",
      "Epoch 182/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1108e-05\n",
      "Epoch 183/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1000e-05\n",
      "Epoch 184/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1081e-05\n",
      "Epoch 185/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.1117e-05\n",
      "Epoch 186/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.0931e-05\n",
      "Epoch 187/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1036e-05\n",
      "Epoch 188/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.0883e-05\n",
      "Epoch 189/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.0896e-05\n",
      "Epoch 190/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.0927e-05\n",
      "Epoch 191/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.1082e-05\n",
      "Epoch 192/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.0792e-05\n",
      "Epoch 193/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.0977e-05\n",
      "Epoch 194/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.0875e-05\n",
      "Epoch 195/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.0733e-05\n",
      "Epoch 196/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.0772e-05\n",
      "Epoch 197/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.0705e-05\n",
      "Epoch 198/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.0673e-05\n",
      "Epoch 199/200\n",
      "155/155 [==============================] - 1s 5ms/step - loss: 1.0891e-05\n",
      "Epoch 200/200\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 1.0672e-05\n",
      "Pretraining time:  140.7062783241272\n",
      "Pretrained weights are saved to results/temp/pretrain_cae_model.h5\n",
      "Initializing cluster centers with k-means.\n",
      "1234/1234 [==============================] - 1s 979us/step\n",
      "Iter 0 : Acc 0.73112 , nmi 0.73872 , ari 0.62793 ; loss= [0 0 0]\n",
      "saving model to: results/temp/dcec_model_quantized1_10.h5\n",
      "saving model to: results/temp/dcec_model_quantized1_150.h5\n",
      "saving model to: results/temp/dcec_model_quantized1_1100.h5\n",
      "Iter 140 : Acc 0.70958 , nmi 0.69348 , ari 0.57435 ; loss= [5.0e-05 4.5e-04 0.0e+00]\n",
      "saving model to: results/temp/dcec_model_quantized1_1150.h5\n",
      "saving model to: results/temp/dcec_model_final_qvout_3_7.h5\n",
      "Pretrain time:   140.7382984161377\n",
      "Clustering time: 11.475939750671387\n",
      "Total time:      152.21423816680908\n",
      "acc = 0.7096, nmi = 0.6935, ari = 0.5744\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    from scipy.optimize import linear_sum_assignment\n",
    "    ind = linear_sum_assignment(np.amax(w) - w)\n",
    "    accuracy = sum([w[i, j] for i, j in zip(*ind)]) * 1.0 / y_pred.size\n",
    "    return accuracy\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 200:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 150:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 100:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 50:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class DCEC(object):\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 filters=[32, 64, 128, 4],\n",
    "                 n_clusters=3,\n",
    "                 alpha=1.0,\n",
    "                 n_level = 1.5,\n",
    "                 scale_factor = 1):\n",
    "\n",
    "        super(DCEC, self).__init__()\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.input_shape = input_shape\n",
    "        self.alpha = alpha\n",
    "        self.pretrained = False\n",
    "        self.y_pred = []\n",
    "\n",
    "        self.cae = CAE(input_shape, filters)\n",
    "        hidden = self.cae.get_layer(name='embedding').output\n",
    "        self.encoder = Model(inputs=self.cae.input, outputs=hidden)\n",
    "\n",
    "        # Define DCEC model\n",
    "        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(hidden)\n",
    "        self.model = Model(inputs=self.cae.input,\n",
    "                           outputs=[clustering_layer, self.cae.output])\n",
    "\n",
    "    def pretrain(self, x, batch_size=256, epochs=200, optimizer='adam', save_dir='results/temp'): #ADJUST EPOCH\n",
    "        print('...Pretraining...')\n",
    "        self.cae.compile(optimizer=optimizer, loss='mse')\n",
    "        from keras.callbacks import CSVLogger\n",
    "        csv_logger = CSVLogger(args.save_dir + '/pretrain_log.csv')\n",
    "\n",
    "        # begin training\n",
    "        t0 = time()\n",
    "        self.cae.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=[csv_logger])\n",
    "        print('Pretraining time: ', time() - t0)\n",
    "        self.cae.save(save_dir + '/pretrain_cae_model.h5')\n",
    "        print('Pretrained weights are saved to %s/pretrain_cae_model.h5' % save_dir)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_weights(self, weights_path):\n",
    "        self.model.load_weights(weights_path)\n",
    "\n",
    "    def extract_feature(self, x):  # extract features from before clustering layer\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        q, _ = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def compile(self, loss=['kld', 'mse'], loss_weights=[1, 1], optimizer='adam'): #previously adam optimizer\n",
    "        self.model.compile(loss=loss, loss_weights=loss_weights, optimizer=optimizer)\n",
    "\n",
    "    def fit(self, x, y=None, batch_size=256, maxiter=2e2, tol=1e-3,                        #ADJUST BATCH_SIZE\n",
    "            update_interval=140, cae_weights=None, save_dir='./results/temp'):\n",
    "\n",
    "        print('Update interval', update_interval)\n",
    "        save_interval = 50 #x.shape[0] / batch_size * 5\n",
    "        print('Save interval', save_interval)\n",
    "\n",
    "        # Step 1: pretrain if necessary\n",
    "        t0 = time()\n",
    "        if not self.pretrained and cae_weights is None:\n",
    "            print('...pretraining CAE using default hyper-parameters:')\n",
    "            print('   optimizer=\\'adam\\';   epochs=200')\n",
    "            self.pretrain(x, batch_size, save_dir=save_dir)\n",
    "            self.pretrained = True\n",
    "        elif cae_weights is not None:\n",
    "            self.cae.load_weights(cae_weights)\n",
    "            print('cae_weights is loaded successfully.')\n",
    "\n",
    "        # Step 2: initialize cluster centers using k-means\n",
    "        t1 = time()\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = MiniBatchKMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        self.y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
    "        y_pred_last = np.copy(self.y_pred)\n",
    "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "\n",
    "        # Step 3: deep clustering\n",
    "        # logging file\n",
    "        import csv, os\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        logfile = open(save_dir + '/dcec_log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'L', 'Lc', 'Lr'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        t2 = time()\n",
    "        loss = [0, 0, 0]\n",
    "        index = 0\n",
    "        for ite in range(int(maxiter)):\n",
    "            if ite % update_interval == 0:\n",
    "                q, _ = self.model.predict(x, verbose=0)\n",
    "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                self.y_pred = q.argmax(1)\n",
    "                if y is not None:\n",
    "                    acc = np.round(accuracy(y, self.y_pred), 5)\n",
    "                    nmi = np.round(normalized_mutual_info_score(y, self.y_pred), 5)\n",
    "                    ari = np.round(adjusted_rand_score(y, self.y_pred), 5)\n",
    "                    loss = np.round(loss, 5)\n",
    "                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, L=loss[0], Lc=loss[1], Lr=loss[2])\n",
    "                    logwriter.writerow(logdict)\n",
    "                    print('Iter', ite, ': Acc', acc, ', nmi', nmi, ', ari', ari, '; loss=', loss)\n",
    "\n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(self.y_pred != y_pred_last).astype(np.float32) / self.y_pred.shape[0]\n",
    "                y_pred_last = np.copy(self.y_pred)\n",
    "                if ite > 0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('Reached tolerance threshold. Stopping training.')\n",
    "                    logfile.close()\n",
    "                    break\n",
    "\n",
    "            # train on batch\n",
    "            #if (index + 1) * batch_size > 64:\n",
    "                #loss = self.model.train_on_batch(x=x[index * batch_size::],\n",
    "                                                 #y=[p[index * batch_size::], x[index * batch_size::]])\n",
    "                #index = 0\n",
    "            #else:\n",
    "                #loss = self.model.train_on_batch(x=x[index * batch_size:(index + 1) * batch_size],\n",
    "                                                 #y=[p[index * batch_size:(index + 1) * batch_size],\n",
    "                                                    #x[index * batch_size:(index + 1) * batch_size]])\n",
    "                #index += 1\n",
    "            \n",
    "            index_array = np.arange(64)\n",
    "            idx = index_array[index * batch_size: min((index+1) * batch_size, 64)]\n",
    "            loss = self.model.train_on_batch(x=np.array(x)[idx], y=[p[idx], np.array(x)[idx]])\n",
    "            index = index + 1 if (index + 1) * batch_size <= 64 else 0\n",
    "\n",
    "            # save intermediate model\n",
    "            if ite % save_interval == 0:\n",
    "                # save DCEC model checkpoints\n",
    "                print('saving model to:', save_dir + '/dcec_model_quantized1_1' + str(ite) + '.h5')\n",
    "                #self.model_save_quantized_weights(save_dir + '/dcec_model_quantized1_6' + str(ite) + '.h5')\n",
    "\n",
    "            ite += 1\n",
    "\n",
    "        # save the trained model\n",
    "        logfile.close()\n",
    "        print('saving model to:', save_dir + '/dcec_model_final_qvout_3_7.h5')\n",
    "        #model_save_quantized_weights(self.model, filename='dcec_model_final_quantized1_6.h5')\n",
    "        self.model.save_weights(save_dir + '/dcec_model_final_qvout_3_7.h5')\n",
    "        t3 = time()\n",
    "        print('Pretrain time:  ', t1 - t0)\n",
    "        print('Clustering time:', t3 - t1)\n",
    "        print('Total time:     ', t3 - t0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # setting the hyper parameters\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='train')\n",
    "    #parser.add_argument('dataset', default='mnist', choices=['mnist', 'usps', 'mnist-test'])\n",
    "    parser.add_argument('--n_clusters', default=12, type=int)\n",
    "    parser.add_argument('--batch_size', default=32, type=int)\n",
    "    parser.add_argument('--maxiter', default=2e2, type=int)\n",
    "    parser.add_argument('--gamma', default=0.1, type=float,\n",
    "                        help='coefficient of clustering loss')\n",
    "    parser.add_argument('--update_interval', default=140, type=int)\n",
    "    parser.add_argument('--tol', default=0.001, type=float)\n",
    "    parser.add_argument('--cae_weights', default=None, help='This argument must be given')\n",
    "    parser.add_argument('--save_dir', default='results/temp')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    print(args)\n",
    "\n",
    "    import os\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "\n",
    "    # load dataset\n",
    "    #from datasets import load_mnist, load_usps\n",
    "    #if args.dataset == 'mnist':\n",
    "        #x, y = load_mnist()\n",
    "    #elif args.dataset == 'usps':\n",
    "        #x, y = load_usps('data/usps')\n",
    "    #elif args.dataset == 'mnist-test':\n",
    "        #x, y = load_mnist()\n",
    "        #x, y = x[60000:], y[60000:]\n",
    "\n",
    "    # prepare the DCEC model\n",
    "    dcec = DCEC(input_shape=(64,1), filters=[48, 32, 32, 5], n_clusters=args.n_clusters, n_level= 0.1,scale_factor=1)\n",
    "    plot_model(dcec.model, to_file=args.save_dir + '/dcec_model_quantized.png', show_shapes=True)\n",
    "    dcec.model.summary()\n",
    "\n",
    "    # begin clustering.\n",
    "    #optimizer = SGD(learning_rate = 0.001, momentum = 0.9)\n",
    "    optimizer = Adam(learning_rate=lr_schedule(0)) #lr_schedule(0)\n",
    "    dcec.compile(loss=['kld', 'mse'], loss_weights=[args.gamma, 1], optimizer=optimizer)\n",
    "    dcec.fit(x, y=y, tol=args.tol, maxiter=args.maxiter,\n",
    "             update_interval=args.update_interval,\n",
    "             save_dir=args.save_dir,\n",
    "             cae_weights=args.cae_weights)\n",
    "    y_pred = dcec.y_pred\n",
    "    print('acc = %.4f, nmi = %.4f, ari = %.4f' % (accuracy(y, y_pred), normalized_mutual_info_score(y, y_pred), adjusted_rand_score(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_true, y_pred):\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    from scipy.optimize import linear_sum_assignment\n",
    "    ind = linear_sum_assignment(np.amax(w) - w)\n",
    "    accuracy = sum([w[i, j] for i, j in zip(*ind)]) * 1.0 / y_pred.size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcec.model.load_weights('results/temp/dcec_model_final_qvout_3_7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 1 : 0.9863460967646186\n",
      "acc 2 : 0.9860012172854534\n",
      "acc 3 : 0.9785180572851806\n",
      "acc 4 : 0.9660726525017135\n",
      "acc 5 : 0.9824019759184933\n",
      "acc 6 : 0.9823353293413174\n",
      "acc 7 : 0.9611163374098464\n",
      "acc 9 : 0.9648039518369868\n",
      "acc 10 : 0.9426674713337356\n",
      "acc 11 : 0.9132653061224489\n",
      "acc 12 : 0.8646477132262052\n",
      "acc 13 : 0.9811437403400309\n",
      "acc 14 : 0.9647887323943662\n",
      "acc 15 : 0.9282004277421326\n",
      "acc 16 : 0.8650497512437811\n",
      "0.9511572507164205\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import sys\n",
    "num = 1\n",
    "datasets = 16\n",
    "accset = np.zeros((datasets))\n",
    "for j in range(0, datasets):\n",
    "    if(j != 7):\n",
    "        for i in range(0, num):\n",
    "            kmeans = MiniBatchKMeans(n_clusters=3, n_init=4)\n",
    "            #spikes1_pred = kmeans.fit_predict(dcec.encoder(getattr(sys.modules[__name__],f\"spikes{j+1}_detected\")))\n",
    "            spikes1_pred = kmeans.fit_predict(third_encoder(getattr(sys.modules[__name__],f\"spikes{j+1}_detected\")))\n",
    "            class1 = np.array(getattr(sys.modules[__name__],f\"spikes{j+1}_class\"))\n",
    "            temp = acc(class1, spikes1_pred)\n",
    "            if( temp > accset[j]):\n",
    "                accset[j] = temp \n",
    "        print(\"acc\", j+1,\":\", accset[j])\n",
    "\n",
    "newset = np.delete(accset,7)\n",
    "print(np.average(newset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eCAE(input_shape=(64, 1), filters=[32, 64, 128, 12], qconfig=[1,8,1,11,1], beta = 0.125, n_level = 0.1):\n",
    "    \n",
    "    num_wbits = qconfig[0]   #weight bits\n",
    "    num_bits  = qconfig[1]   #relu bits (input bits)\n",
    "    num_int   = qconfig[2]   #relu integer bits (input integer bits)\n",
    "    num_abits = qconfig[3]   #quantization bits (ADC bits)\n",
    "    num_aint  = qconfig[4]   #quantization integer bits\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape))\n",
    "    model.add(QConv1D(filters[0], 1, strides=1, padding='same', kernel_quantizer=quantized_bits(num_wbits, 0, 1, False, alpha = beta), use_bias = False, name='conv1'))\n",
    "    model.add(GaussianNoise(stddev=n_level))\n",
    "    \n",
    "    model.add(QConv1D(filters[1], 1, strides=1, padding='same', kernel_quantizer=quantized_bits(num_wbits, 0, 1, False, alpha = beta), use_bias = False, name='conv2'))\n",
    "    model.add(GaussianNoise(stddev=n_level))\n",
    "    \n",
    "    model.add(QConv1D(filters[2], 1, strides=1, padding='same', kernel_quantizer=quantized_bits(num_wbits, 0, 1, False, alpha = beta), use_bias = False, name='conv3'))\n",
    "    model.add(GaussianNoise(stddev=n_level))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(QDense(units=filters[3], kernel_quantizer=quantized_bits(num_wbits, 0, 1, False, alpha = beta), use_bias = False,name='qdense1'))\n",
    "    model.add(GaussianNoise(stddev=1))\n",
    "    \n",
    "    model.add(QActivation(quantized_bits3(num_abits, num_aint, 0, False, alpha = beta), name='embedding'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/vincent/anaconda3/envs/spikesort310/lib/python3.10/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer HeNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#K.clear_session()\n",
    "\n",
    "new_encoder = eCAE(input_shape=(64,1), filters=[48, 32, 32, 5], n_level = 1)\n",
    "#optimizer = Adam(learning_rate=lr_schedule(0)) #lr_schedule(0)\n",
    "optimizer = Adam()\n",
    "new_encoder.compile(loss=['kld', 'mse'], loss_weights=[0.1, 1], optimizer=optimizer)\n",
    "\n",
    "input_layer = new_encoder.get_layer(name='conv1').input\n",
    "hidden = new_encoder.get_layer(name='embedding').output\n",
    "third_encoder = Model(inputs=input_layer, outputs=hidden)\n",
    "\n",
    "third_encoder.load_weights('results/temp/dcec_model_final_qvout_3_6.h5',by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential K-Means (Hardware Implemented Algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential K-Means\n",
    "def sequential_k_means_adc(encoded):\n",
    "    d = [0] * 6\n",
    "    encoded = np.array(encoded)\n",
    "    existing_cluster = [0, 1, 2]\n",
    "    c = []\n",
    "    class1_sequential = [0, 1, 2]\n",
    "    c.append(encoded[0])\n",
    "    c.append(encoded[1])\n",
    "    c.append(encoded[2])\n",
    "\n",
    "    c = np.array(c)\n",
    "    count = [1, 1, 1]\n",
    "\n",
    "    for i in range(3, len(encoded)):\n",
    "        #Calculate Euclidean Distance\n",
    "        d[0] = sum(abs(c[0]-encoded[i]))\n",
    "        d[1] = sum(abs(c[1]-encoded[i]))\n",
    "        d[2] = sum(abs(c[2]-encoded[i]))\n",
    "        #Existing Clusters are weighted\n",
    "        if i < 13:\n",
    "            d[3] = sum(abs(c[0]-c[1]))\n",
    "            d[4] = sum(abs(c[0]-c[2]))\n",
    "            d[5] = sum(abs(c[1]-c[2]))\n",
    "            decide = np.argmin(d)\n",
    "        else :\n",
    "            if (len(d) != 3) :\n",
    "                d = np.delete(d, 3)\n",
    "                d = np.delete(d, 3)\n",
    "                d = np.delete(d, 3)\n",
    "            decide = np.argmin(d)\n",
    "        #print(d)\n",
    "        #print(decide)\n",
    "    \n",
    "        #Update Cluster Means\n",
    "        if decide < 3 :\n",
    "            #if count[decide] < 128 :\n",
    "            if i < 112 :\n",
    "                #c[decide] = 15*((c[decide]*1/16)//1) + ((encoded[i]*1/16)//1)\n",
    "                for j in range(0,5):\n",
    "                    if(c[decide,j]%4 == 0):\n",
    "                        temp = np.floor(c[decide,j]/4)*3\n",
    "                    elif(c[decide,j]%4 == 1):\n",
    "                        temp = np.floor(c[decide,j]/4)*3+1\n",
    "                    else:\n",
    "                        temp = np.floor(c[decide,j]/4)*3+2\n",
    "                    if(encoded[i,j]%4 == 0 or encoded[i,j]%4 == 1 ):\n",
    "                        temp2 = np.floor(encoded[i,j]/4)\n",
    "                    else:\n",
    "                        temp2 = np.floor(encoded[i,j]/4)+1\n",
    "                    c[decide,j] = temp+temp2\n",
    "                #c[decide] = np.round(c[decide]/4)*3 + np.round(encoded[i]*1/4)\n",
    "            count[decide] = count[decide]+1\n",
    "            class1_sequential = np.append(class1_sequential, decide)\n",
    "            #print(c)\n",
    "        #Merge Similar Clusters\n",
    "        elif decide == 3 :\n",
    "            temp = 0\n",
    "            temp1 = 1\n",
    "            #c[temp] = ((c[temp]/2)//1) + ((c[temp1]/2)//1)\n",
    "            c[temp] = np.round(c[temp]/2) + np.round(c[temp1]/2)\n",
    "            c[temp1] = encoded[i]\n",
    "            count[temp] = count[temp] + count[temp1]\n",
    "            count[temp1] = 1\n",
    "            class1_sequential[class1_sequential == temp1] = temp\n",
    "            class1_sequential = np.append(class1_sequential, temp1)\n",
    "            #print(c)\n",
    "        elif decide == 4 :\n",
    "            temp = 0\n",
    "            temp1 = 2\n",
    "            #c[temp] = ((c[temp]/2)//1) + ((c[temp1]/2)//1)\n",
    "            c[temp] = np.round(c[temp]/2) + np.round(c[temp1]/2)\n",
    "            c[temp1] = encoded[i]\n",
    "            count[temp] = count[temp] + count[temp1]\n",
    "            count[temp1] = 1\n",
    "            class1_sequential[class1_sequential == temp1] = temp\n",
    "            class1_sequential = np.append(class1_sequential, temp1)\n",
    "            #print(c)\n",
    "        elif decide == 5 :\n",
    "            temp = 1\n",
    "            temp1 = 2\n",
    "            #c[temp] = ((c[temp]/2)//1) + ((c[temp1]/2)//1)\n",
    "            c[temp] = np.round(c[temp]/2) + np.round(c[temp1]/2)\n",
    "            c[temp1] = encoded[i]\n",
    "            count[temp] = count[temp] + count[temp1]\n",
    "            count[temp1] = 1\n",
    "            class1_sequential[class1_sequential == temp1] = temp\n",
    "            class1_sequential = np.append(class1_sequential, temp1)\n",
    "            #print(c)\n",
    "    #print(c)\n",
    "    return class1_sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 1 : 0.9830558858501783\n",
      "acc 2 : 0.9860035735556879\n",
      "acc 3 : 0.9782141761276465\n",
      "acc 4 : 0.959123610202747\n",
      "acc 5 : 0.9760263709919089\n",
      "acc 6 : 0.9821428571428571\n",
      "acc 7 : 0.9650199447683339\n",
      "acc 9 : 0.9559228650137741\n",
      "acc 10 : 0.941923774954628\n",
      "acc 11 : 0.9233082706766917\n",
      "acc 12 : 0.8375650045885592\n",
      "acc 13 : 0.9788602941176471\n",
      "acc 14 : 0.9608904933814681\n",
      "acc 15 : 0.9263222256190767\n",
      "acc 16 : 0.8616312281815297\n",
      "0.9477340383448489\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "num = 1\n",
    "datasets = 16\n",
    "accset = np.zeros((datasets))\n",
    "for j in range(0, datasets):\n",
    "    if(j != 7):\n",
    "        for i in range(0, num):\n",
    "            spikes1_pred = sequential_k_means_adc(third_encoder(getattr(sys.modules[__name__],f\"spikes{j+1}_detected\")))\n",
    "            class1 = np.array(getattr(sys.modules[__name__],f\"spikes{j+1}_class\"))\n",
    "            temp = acc(class1, spikes1_pred)\n",
    "            if( temp > accset[j]):\n",
    "                accset[j] = temp \n",
    "        print(\"acc\", j+1,\":\", accset[j])\n",
    "\n",
    "newset = np.delete(accset,7)\n",
    "print(np.average(newset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spikesort310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
